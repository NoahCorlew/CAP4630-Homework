{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP4630_HW4_P1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIuPlnjkyZxHyeHJPTnbXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahCorlew/CAP4630-Homework/blob/master/HW4/CAP4630_HW4_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-NyNEomrs0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Error(Exception):\n",
        "  pass\n",
        "\n",
        "class InputMatSquare(Error):\n",
        "  pass\n",
        "\n",
        "class KernalSquare(Error):\n",
        "  pass\n",
        "\n",
        "class KernalSize(Error)\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJP4dg09h-5q",
        "colab_type": "code",
        "outputId": "e3daaf7a-cbf8-4136-a8e5-33780d56e250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import numpy\n",
        "\n",
        "def conv2d(input_mat, kernal_mat):\n",
        "  # Calling custom exceptions\n",
        "  if (input_mat.shape[0] != input_mat.shape[1]):\n",
        "    raise InputMatSquare(\"input_mat is not a square matrix\")\n",
        "  \n",
        "  if (kernal_mat.shape[0] != kernal_mat.shape[1]):\n",
        "    raise KernalSquare(\"kernal_mat is not a square matrix\")\n",
        "  \n",
        "  # Case where the kernal and input are the same size\n",
        "  if (input_mat.shape[0] == kernal_mat.shape[0]):\n",
        "    return numpy.tensordot(input_mat, kernal_mat)\n",
        "\n",
        "\n",
        "  # Getting sizes of input and kernal\n",
        "  input_size = input_mat.shape[0]\n",
        "  kernal_size = kernal_mat.shape[0]\n",
        "  \n",
        "  # Getting the submatrices that will have the kernal dotted with\n",
        "  subM = []\n",
        "  for i in range(0, input_size - 1):\n",
        "    for j in range(0, input_size - 1):\n",
        "      subM.append(input_mat[i:kernal_size + i, j:kernal_size + j])\n",
        "\n",
        "  # Calculating the dot product of the submatrices and kernal\n",
        "  dotp = []\n",
        "  for i in range(0, len(subM)):\n",
        "    dotp.append(numpy.tensordot(subM[i], kernal_mat))\n",
        "\n",
        "  # Creating output_mat using the results of the above calculations\n",
        "  k = 0\n",
        "  output_mat = numpy.zeros(shape=(input_size - 1, input_size - 1))\n",
        "  for i in range(0, input_size - 1):\n",
        "    for j in range(0, input_size - 1):\n",
        "      output_mat[i][j] = dotp[k]\n",
        "      k += 1\n",
        "\n",
        "  return output_mat.astype(int)\n",
        "\n",
        "input_mat = numpy.array([[1, 0, 0, 0],\n",
        "                      [0, 1, 0, 0],\n",
        "                      [0, 0, 1, 0],\n",
        "                      [0, 0, 0, 1]])\n",
        "\n",
        "kernal_mat = numpy.array([[1, 0], [0, 1]])\n",
        "\n",
        "conv2d(input_mat, kernal_mat)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 0, 0],\n",
              "       [0, 2, 0],\n",
              "       [0, 0, 2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    }
  ]
}