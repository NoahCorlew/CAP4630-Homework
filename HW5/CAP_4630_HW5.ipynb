{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP 4630 HW5",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUBvxlzj0jybY0WvlZcu+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahCorlew/CAP4630-Homework/blob/master/HW5/CAP_4630_HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zFC5zDoz6vk",
        "colab_type": "text"
      },
      "source": [
        "# General Concepts\n",
        "\n",
        "What I learned in AI School is..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqkJ3Vf-0K2e",
        "colab_type": "text"
      },
      "source": [
        "### Artifical Intelligence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyGOI7cu0YKD",
        "colab_type": "text"
      },
      "source": [
        "Aritifical Intelligence is a term used for a piece of software that can take an input, read the input, then  return an output that simulates a human analyzing the input in a specific way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECUhWjSf0KwG",
        "colab_type": "text"
      },
      "source": [
        "### Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tpb2IwI0Ysi",
        "colab_type": "text"
      },
      "source": [
        "Machine learning is similar to Artificial Intelligence in that it takes an input and produces an output, however it differs in the fact that it can learn and change to create more correct output, the end goal being to be able to make predictions and create output based on unfamiliar data. In general, machine learning uses patterns in sample data and uses it to make predictions about new data.\n",
        "\n",
        "There are three types of machine learning, Supervised, Unsupervised, and Reinforcement. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpxIxGV56jQ",
        "colab_type": "text"
      },
      "source": [
        "#### Reinforcment:\n",
        "Reinforcement Learning is when the model is provided data without labels and is rather given a goal to achieve. The model will attempt to learn how to achieve that goal. When the model gets closer to the goal, the actions it took to get to that point are reinforced, and most of the time repeated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSheKxaq5yNE",
        "colab_type": "text"
      },
      "source": [
        "#### Supervised\n",
        "Supervised learning is when the model is provided with training data that has information attached to each piece of data describing what it is in a way the algorithm can read. The goal being to be able to be given pieces of data and make predictions on what the label on that new data should be. Our assignment requiring us to use TensorFlow to organize written numbers is an example of supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMt5Bf6t5XH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%matplotlib inline\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "average0 = np.full((28, 28), 0)\n",
        "average1 = np.full((28, 28), 0)\n",
        "average2 = np.full((28, 28), 0)\n",
        "average3 = np.full((28, 28), 0)\n",
        "average4 = np.full((28, 28), 0)\n",
        "average5 = np.full((28, 28), 0)\n",
        "average6 = np.full((28, 28), 0)\n",
        "average7 = np.full((28, 28), 0)\n",
        "average8 = np.full((28, 28), 0)\n",
        "average9 = np.full((28, 28), 0)\n",
        "\n",
        "\n",
        "\n",
        "amnt = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "def addMatri(result, add):\n",
        "    result = np.add(result, add)\n",
        "    return result\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for n in range(5000):\n",
        "    amnt[train_labels[n]] = 1 + amnt[train_labels[n]]\n",
        "    if (train_labels[n] == 0):\n",
        "        average0 = addMatri(average0, train_images[n])\n",
        "    if (train_labels[n] == 1):\n",
        "        average1 = addMatri(average1, train_images[n])\n",
        "    if (train_labels[n] == 2):\n",
        "        average2 = addMatri(average2, train_images[n])        \n",
        "    if (train_labels[n] == 3):\n",
        "        average3 = addMatri(average3, train_images[n])\n",
        "    if (train_labels[n] == 4):\n",
        "        average4 = addMatri(average4, train_images[n])\n",
        "    if (train_labels[n] == 5):\n",
        "        average5 = addMatri(average5, train_images[n])\n",
        "    if (train_labels[n] == 6):\n",
        "        average6 = addMatri(average6, train_images[n])\n",
        "    if (train_labels[n] == 7):\n",
        "        average7 = addMatri(average7, train_images[n])\n",
        "    if (train_labels[n] == 8):\n",
        "        average8 = addMatri(average8, train_images[n])\n",
        "    if (train_labels[n] == 9):\n",
        "        average9 = addMatri(average9, train_images[n])\n",
        "\n",
        "\n",
        "for n in range(10):\n",
        "    ax = plt.subplot(4, 4, n + 1)\n",
        "    amntMatr = np.full((28, 28), amnt[n])\n",
        "    if (n == 0):\n",
        "        plt.imshow(average0 / amntMatr)\n",
        "    if (n == 1):\n",
        "        plt.imshow(average1 / amntMatr)\n",
        "    if (n == 2):\n",
        "        plt.imshow(average2 / amntMatr)\n",
        "    if (n == 3):\n",
        "        plt.imshow(average3 / amntMatr)\n",
        "    if (n == 4):\n",
        "        plt.imshow(average4 / amntMatr)\n",
        "    if (n == 5):\n",
        "        plt.imshow(average5 / amntMatr)\n",
        "    if (n == 6):\n",
        "        plt.imshow(average6 / amntMatr)\n",
        "    if (n == 7):\n",
        "        plt.imshow(average7 / amntMatr)\n",
        "    if (n == 8):\n",
        "        plt.imshow(average8 / amntMatr)\n",
        "    if (n == 9):\n",
        "        plt.imshow(average9 / amntMatr)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nflVy_KB5g3N",
        "colab_type": "text"
      },
      "source": [
        "#### Unsupervised:\n",
        "Unsupervised learning is when the model is provided with training data without labels, the goal being to group the data together in a logical way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbTdkdj20Ki7",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning\n",
        "Just as machine learning is a subset of artificial intelligence, deep learning is a subset of machine learning. The main difference is that deep learning uses neural networks and machine learning does not.\n",
        "Our assignment on predicting plots used a deep-learning API Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8smo3OvC2xB",
        "colab_type": "text"
      },
      "source": [
        "# Basic Concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpYAnT6ODMFL",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "Linear Regression attempts to predict, using a set of values each with an independent and dependent variable, the dependent variable that corresponds to a new independent variable. In other words, the goal of a linear regression algorithm is to find the line of best fit for a set of data.\n",
        "We used Keras in assignment three to train a model to preform linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFwsZkHmDPiR",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is similar to linear regression, but differs in that logistic regression looks at multiple independent variables rather than just one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQwkTkG5DToO",
        "colab_type": "text"
      },
      "source": [
        "### Gradient \n",
        "\n",
        "Gradient is how we capture all partial derivatives of a multi-variable function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6JvPPddDWlS",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Gradient Descent is an optimization algorithm for minimizing loss using partial derivatives of the cost function. \n",
        "\n",
        "There are 3 types of gradient descent:\n",
        "* Stochastic Gradient Descent: When gradient descent is run, treating each batch as a seperate piece of data\n",
        "\n",
        "* Full-batch Gradient Descent: When gradient descent is run with only one batch\n",
        "\n",
        "* Mini-batch Gradient Descent: A mix between Full-batch and Stochastic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC5yShGYJg9R",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model\n",
        "\n",
        "A model is made up of a neural network, which is a set of layers with weighted neurons and weighted edges connecting them not unlike an adjacency matrix. These layers are used and modified while the model is running with something called forward propagation. Forward propagation is when information is passed through the neural network, and the weights and biases of the neurons and edges are changed as the model changes. This causes the network to have a bias towards better output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbPItXlWUEW",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "A CNN is a form of a neural network, the distinction of a CNN is that it is convolving with images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui8ucWdcqLXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "layers = tf.keras.layers.Dense(1, activation = \"sigmoid\", input_shape = (2,))\n",
        "model.add(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEmmIy7aqNl-",
        "colab_type": "text"
      },
      "source": [
        "# Compiling a model\n",
        "As seen in homework 4, when compiling the model, multiple layers are added separately with different activation functions. In the bellow code snippet, relu and sigmoid functions were used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjVxUdQ6qO5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uT6rFPP3vsC",
        "colab_type": "text"
      },
      "source": [
        "In the bellow code snippet, you can see the model being compiled with binary crossentropy as the loss function, RMSprop as the optimizer and accuracy as the metric to be evaluated during tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03qN_rHt3ys8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=optimizers.RMSprop(lr=2e-5), \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU3f4F1Xq7_C",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions\n",
        "A loss function is a function that measures the difference between the expected value and the target for the training examples, in the case that the problem is a supervised learning one. This difference is used to measure how well a model is doing, and minimizing loss is one of the main goals of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA4e_PYZq9H8",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers\n",
        "The goal of an optimizer is to look at the loss function and determine how the network will be updated, most use some form or variation of gradient decent as it is the most widely used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qo8af72rDAv",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate\n",
        "In machine learning, the learning rate is used to set how often the weights and biases of nodes and edges in a network are changed. The higher the learning rate, the faster the model learns and changes, but it is also more likely to make mistakes and bad assumptions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHKOoT6yrFa2",
        "colab_type": "text"
      },
      "source": [
        "# Training a model\n",
        "Training a model is the cumulation of all of the above work, and is when the model is run. In homework 3, the following is a code snippet of that. An epoch is the term used for when the entire set of data is passed through and back through a neural network once, while batch size is number of training examples within a single batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbRFtW3lrNic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(training_data, training_labels, epochs = 80, batch_size = 500)\n",
        "weights, bias = layers.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xcy7Yx2rQ7G",
        "colab_type": "text"
      },
      "source": [
        "# Overfiting and Underfiting data\n",
        "When you overfit data, you get results that have a high accuracy just for the set of training data, but not necessarily much else. When a model is underfit, that means that the model was given too high of a bias and a too low variance. The result is data that does not correspond with the true data. Overall, an overfit model will have low training error but high testing error, and a model that is underfit will have high error on both fronts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihb-qPw3rYdd",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning a pretrained model\n",
        "The first step of finetuning a model is to verify the data to determine if the model is improving or not. From here you can continue to finetune the model. To do this, you freeze different layers of the model, or make it so that that layer is not to be trained. Then you recreate the model with the frozen layers and determine if the model has improved from before freezing the layers. Homework 4 was focused on this, and the freezing of different layers can be seen in the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkVMx8N8qMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv2':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wuNER8Yrg0b",
        "colab_type": "text"
      },
      "source": [
        "We replaced the layer to be frozen, in the two parts of the homework to determine which was more successful in finetuning the mode. In the above snippet, the layer is block5_conv2.\n",
        "\n",
        "Layers can also be included as dropout layers. These dropout layers help avoid overfitting by removing unneeded nodes and edges by randomly having these nodes ouptut nothing.\n",
        "\n",
        "Another way to fine tune a model is with noise layers, which again helps to prevent overfitting. "
      ]
    }
  ]
}